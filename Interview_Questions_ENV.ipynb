{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAYVLRmeh29WLFNU/jT2eJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <u><center>\tInterview Questions (Optional)</u>\n",
        "* Authored By: Eric N. Valdez\n",
        "* Date: 3/2/24"
      ],
      "metadata": {
        "id": "5YdIhZid00c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Interview Questions`\n",
        "This optional assignment provides an opportunity for you to continue your growth and learning. It will reinforce concepts from the previous lesson(s), but is not required and the skills gained from this assignment will not be found on the belt exam.\n",
        "\n",
        "Practice and tinker with this assignment if you have time. However, if you do not feel it is an opportunity you have time for in your current schedule, we recommend that you skip this optional assignment.\n",
        "Interview Questions\n",
        "In order to prepare for technical interviews, each week we will go over sample interview questions that relate to the material we have covered that week. This assignment is optional! If you choose to complete it, your task is to answer each of the questions below.\n",
        "\n",
        "These questions can typically be answered in a couple of sentences, but I encourage you to think about how you would answer these out loud - or even practice answering them out loud - since that is how you would answer them in an actual interview.\n",
        "\n",
        "It also might be a good idea to keep these somewhere to review for after graduation when you are job hunting! Creating flashcards with potential technical interview questions is a good strategy.\n"
      ],
      "metadata": {
        "id": "vcCnPzO-0esW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Dimensionality Reduction Interview Questions`"
      ],
      "metadata": {
        "id": "xcLTQt441YzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.   Why would you want to use dimensionality reduction techniques to transform your data before training?"
      ],
      "metadata": {
        "id": "NdEJ5n4c1str"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### Less dimensions lead to less computation/training time\n",
        "- Dimensionality reduction (DR) is a pre-processing step that removes redundant features, noisy data, and irrelevant data from a dataset.\n",
        "- DR can be performed after data cleaning and scaling, but before training"
      ],
      "metadata": {
        "id": "fVFYB0HZ3D1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Why would you want to avoid dimensionality reduction techniques to transform your data before training?\n",
        "\n"
      ],
      "metadata": {
        "id": "PQTAIrOJ1w1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work.\n",
        "2. It may need a lot of processing power.\n",
        "3. Interpreting transformed characteristics might be challenging.\n",
        "4. The independent variables become harder to comprehend as a result."
      ],
      "metadata": {
        "id": "fvubLrQf3FA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Name a popular dimensionality reduction algorithm and briefly describe it."
      ],
      "metadata": {
        "id": "7ll_eICR1w_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA)\n",
        "\n",
        "One of the leading linear techniques of dimensionality reduction. This method performs a direct mapping of the data to a lesser dimensional space in a way that maximizes the variance of the data in the low-dimensional representation."
      ],
      "metadata": {
        "id": "6GjX4ahP3FuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. After doing dimensionality reduction, can you transform the data back into the original feature space? How?"
      ],
      "metadata": {
        "id": "i6Z_VERJ1xKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is `'Yes'` because the RBF kernel function temporarily projects the 2-dimensional data into a new higher-dimensional feature space where the classes become linearly separable and then the algorithm projects that higher-dimensional data back into the 2-dimensional data which can be plotted in a 2D plot."
      ],
      "metadata": {
        "id": "bkGQP7Sy3GgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. How do you select the number of principal components needed for PCA?\n"
      ],
      "metadata": {
        "id": "ENA7WeKD1xTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PCA, choose the smallest number of components that still capture most of the information in your data. A common approach is to pick enough components to cover about 95% of the total data variance."
      ],
      "metadata": {
        "id": "Cu0ivAkV3HR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. When would you use PCA?\n"
      ],
      "metadata": {
        "id": "wQn-d1vt1xdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically used as an intermediate step in data analysis when the number of input variables is too large for useful analysis."
      ],
      "metadata": {
        "id": "nZ-s9wrN3IEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Why would you perform PCA even if you don't have a lot of features?"
      ],
      "metadata": {
        "id": "qmwFxQIi1xnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The only way PCA is a valid method of feature selection is if the most important variables are the ones that happen to have the most variation in them\n",
        "- The resulting projected data are essentially linear combinations of the original data capturing most of the variance in the data.\n"
      ],
      "metadata": {
        "id": "fcRU-gyN3Ivn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. In what cases would you NOT use PCA?"
      ],
      "metadata": {
        "id": "2XI-RbTX1xws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is not suitable in many cases:\n",
        "- For example, if all the components of PCA have quite a high variance, there is no 'good' universal stopping rule that allows you to discard some exact 'k' Principal Components, with a guarantee that there will be no 'significant' ( for your application) loss of data in the compression.\n",
        "- Secondly, it is not suitable for some classification problem.\n",
        "- Suppose there are 2 classes of your data, but the within class variance is very high as compared to between class variance, PCA might result in you discarding the very information that separates your two classes.\n",
        "- In other words, if your data is noisy, and the noise variance is more than the variance between means of the two classes, then PCA will keep the noise components, and let you discard the distinguishing component (This is expected, since PCA is unsupervised).\n",
        "- This makes data in the lower space to be indistinguishable."
      ],
      "metadata": {
        "id": "Aj_X9HX83Jcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. What is the geometric interpretation of an eigenvector and eigenvalue?"
      ],
      "metadata": {
        "id": "U2yJbPCf1x6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In linear algebra, eigenvectors and eigenvalues\n",
        " are used to characterize a transformation.\n",
        "- Geometrically, vectors are quantities with magnitude and direction, and are often represented as arrows.\n",
        "- A linear transformation rotates, stretches, or shears the vectors it acts on.\n",
        "- An eigenvector is a vector that is only stretched, with no rotation or shear, when a linear transformation is applied to it.\n",
        "- The corresponding eigenvalue is the factor by which an eigenvector is stretched or squished.\n",
        "- If the eigenvalue is negative, the eigenvector's direction is reversed.\n",
        "- For example, in geology, eigenvectors describe the orientation of an ellipsoid, and eigenvalues describe its shape.\n",
        "- In quantum mechanics, eigenvalues and eigenvectors are used to describe the energy levels and corresponding wavefunctions of quantum systems."
      ],
      "metadata": {
        "id": "IvbLGViu3KHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. What is the algebraic interpretation of an eigenvector and eigenvalue?"
      ],
      "metadata": {
        "id": "o4fBBICx1yE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Eigenvalues are the special set of scalar values that is associated with the set of linear equations most probably in the matrix equations.\n",
        "- The eigenvectors are also termed as characteristic roots.\n",
        "- It is a non-zero vector that can be changed at most by its scalar factor after the application of linear transformations."
      ],
      "metadata": {
        "id": "_gs23P8Y3K5G"
      }
    }
  ]
}